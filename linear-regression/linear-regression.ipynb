{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归算法基本介绍\n",
    "\n",
    "在学习线性回归的知识点之前，我们先了解一下监督学习的概念。监督学习比较正式的定义是，给定一个训练集，学习到一个函数h：\n",
    "\n",
    "$χ↦y$\n",
    "\n",
    "h(x)是对对应值y的一个很好的估计,由于历史存在的问题，将这个函数h成为假设(hypothesis)。\n",
    "\n",
    "如果我们估计的值是个连续的值，我们将我们的学习问题称为回归，如果估计的值是一个离散的值，我们将我们的学习问题成为分类。\n",
    "\n",
    "线性回归指的是用自变量x的一次函数来估计结果y的值。\n",
    "\n",
    "\n",
    "$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1}+\\theta_{2}x_{2}$\n",
    "\n",
    "上述中 $\\theta_{i}$为参数(也称为权重)，为了进一步简化我们的表示，我们假设 $x_0=1$,因此上式又可以写成：  \n",
    "\n",
    "$h_{\\theta}(x)=\\sum_{i=0}^{n}\\theta_ix_i=\\theta^Tx$\n",
    "\n",
    "给定一个数据集的情况下，怎样选择参数$\\theta$，使得假设能够比较接近y呢？这里，我们引入了成本函数(cost function)，它衡量了假设和y值得接近程度，即最小平方(最小二乘法)成本函数(least square cost function)  \n",
    "\n",
    "$J(\\theta)= \\frac{1}{2}\\sum_{i=0}^{m}(h_{\\theta}(x^{(i)}-y^{(i)})^2$\n",
    "\n",
    "\n",
    "##### 最小均方算法(LMS, Least Mean Square)  \n",
    "我们的目标是选择合适的$\\theta$,去最小化成本函数$J(\\theta)$。我们从一个猜想的初始$\\theta$开始，逐步去更新$\\theta$，使得$J(\\theta)$更小，最终收敛到一个$\\theta$值，使得$J(\\theta)$最小。  \n",
    "  \n",
    "更加具体一点的是，考虑梯度下降算法(gradient descent)，从一个初始的$\\theta$值开始，不断重复地去做如下的更新：  \n",
    "\n",
    "$\\theta_{j} := \\theta_{j}-\\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta)$\n",
    "\n",
    "上述的更新针对所有的$j=0,1,2...n$，这里$\\alpha$是学习率，在这里每一步选择$J(\\theta)$的最陡的方向进行$\\theta$的更新。  \n",
    "\n",
    "当只有一个样本的时候，有如下关系成立：\n",
    "\n",
    "$\\frac{\\partial}{\\theta_{i}}J(\\theta)=\\frac{\\partial}{\\partial\\theta_{i}}\\frac{1}{2}(h_{\\theta}(x)-y)^2$\n",
    "\n",
    "$=2\\cdot \\frac{1}{2}(h_{\\theta}(x)-y)\\cdot \\frac{\\partial}{\\partial \\theta_{i}}(h_{\\theta(x)}-y)$\n",
    "\n",
    "$=(h_{\\theta(x)}-y)\\cdot x_{i}$\n",
    "\n",
    "\n",
    "因此，当只有一个样本的时候，$\\theta$的更新规则如下：\n",
    "\n",
    "$\\theta_{j} := \\theta_{j}+\\alpha(y^{(i)}-h_\\theta(x^{(i)})x_j^{(i)}$\n",
    "\n",
    "这个规则也叫做最小均方更新规则，这个规则看上去很自然也很直观，比如更新的幅度正比于error项$y^{(i)}-h_\\theta(x^{(i)}$，当预测的值和y值比较match的时候，误差项比较小，$\\theta$更新的幅度也小，当预测的值和y值相差比较大的时候，说明模型参数还没有能很好地进行目标预测，$\\theta$更新的幅度也大。\n",
    "\n",
    "当有多个样本的时候，有两种方法可以对上述规则进行扩展，第一种方法称为批量梯度下降法(batch gradient decent),具体更新规则如下：  \n",
    "迭代如下公式直至收敛\n",
    "\n",
    "$\\theta_{j} := \\theta_{j}+\\alpha \\sum_{i=1}^{m}(y^{(i)}-h_\\theta(x^{(i)})x_j^{(i)}\\  (for \\  every \\ j)$\n",
    "\n",
    "需要注意的是，梯度下降法容易找到局部最小点，但是在这里提出的线性回归的优化问题只有一个全局的，而没有局部的最优值。因此在这里梯度下降法总是收敛到全局的最小值。在这里`$J$`是一个凸的二次函数，可以用梯度下降法去优化凸二次函数。  \n",
    "第二种方法称为随机梯度下降法(stochastic gradient decent),具体更新规则如下：  \n",
    "迭代如下公式：  \n",
    "\n",
    "$\n",
    "for\\ i=1\\ to\\ m: \\{ \n",
    "\\theta_{j} := \\theta_{j}+\\alpha (y^{(i)}-h_\\theta(x^{(i)})x_j^{(i)}\\  (for \\  every \\ j)\\}\n",
    "$\n",
    "\n",
    "在随机梯度下降的过程中，我们每次碰到一个样本，$\\theta$更新的时候只针对当前的样本去算梯度。批量梯度下降是扫描整个数据集然后更新参数，这是一个非常耗费的运算，而随机梯度更新一次参数的运算只针对单个样本，计算很快，所以能更快的接近最值点。需要注意的是，随机梯度下降不能达到最值点，在最后会在最值点震荡，但在很多问题中这样的优化策略已经足够实用。因此在实际场景中，随机梯度下降法比批量梯度下降法更实用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归的概率解释：\n",
    "\n",
    "当面临线性回归的时候，为什么将最小二乘作为成本函数是合适的呢？在本部分，将给出一个概率解释，就理解为什么最小二乘作为线性回归的成本函数是很自然的一个算法。假设我们的目标变量和输入变量通过如下的等式关联起来:\n",
    "  \n",
    "  $y^{(i)}=\\theta ^T x^{(i)}+\\epsilon ^{(i)}$\n",
    "\n",
    "  其中$\\epsilon ^{(i)}$为误差项，可以将误差归结为是由于没有纳入到建模中的一些变量（特征），或者仅仅是由于噪声引起的。我们进一步假设$\\epsilon ^{(i)}$服从IID(independently and identically distributed)的均值为0，方差为$\\sigma$的高斯分布，写作 $\\epsilon ^{(i)}\\sim N(0,\\sigma^2)$ 。$\\epsilon ^{(i)}$ 的概率密度为：\n",
    "  \n",
    "$p(\\epsilon^{(i)})=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(\\epsilon^{(i)})^2}{2\\sigma^2})$\n",
    "\n",
    "因此下面的等式也会成立：\n",
    "  \n",
    "$p(y^{(i)}|x^{(i)};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$\n",
    "\n",
    "  给定一个数据集，它的似然函数(样本的联合分布)可以表示为：\n",
    "  \n",
    "$L(\\theta)=L(\\theta;X,\\vec y)=p(\\vec y |X;\\theta)$\n",
    "\n",
    "   由于我们假设$\\epsilon ^{(i)}$服从IID的高斯分布，因此上述的公式也可以写为:\n",
    "   \n",
    "$L(\\theta)=\\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$\n",
    "  现在我们考虑怎么选择合适的$\\theta$来最大化$L(\\theta)$。这里面我们采用了一个等价的方法，通过最大化$logL(\\theta)$\n",
    "\n",
    "$l(\\theta)=logL(\\theta)$\n",
    "\n",
    "$= log\\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$\n",
    "\n",
    "$= \\sum_{i=1}^{m}log\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$\n",
    "\n",
    "$= mlog\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{1}{\\sigma^2}\\frac{1}{2}\\sum_{i=1}^{m}(y^{(i)}-\\theta^Tx^{(i)})^2$\n",
    "\n",
    "根据上式，最大化$l(\\theta)$也就是最小化$\\frac{1}{2}\\sum_{i=1}^{m}(y^{(i)}-\\theta^Tx^{(i)})^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-25T08:18:45.532165Z",
     "iopub.status.busy": "2022-08-25T08:18:45.531449Z",
     "iopub.status.idle": "2022-08-25T08:18:45.545691Z",
     "shell.execute_reply": "2022-08-25T08:18:45.543824Z",
     "shell.execute_reply.started": "2022-08-25T08:18:45.532106Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T08:18:49.973652Z",
     "iopub.status.busy": "2022-08-25T08:18:49.973178Z",
     "iopub.status.idle": "2022-08-25T08:18:50.224270Z",
     "shell.execute_reply": "2022-08-25T08:18:50.223039Z",
     "shell.execute_reply.started": "2022-08-25T08:18:49.973615Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "'''\n",
    "make dummy regression data\n",
    "sklearn.datasets.make_regression \n",
    "\n",
    "params:\n",
    "    @random_state, int or None\n",
    "    随机数状态，类似random seed，在多次的不同调用中可以传入相同的数来生成可重复的输出\n",
    "    \n",
    "    @n_targets : int, optional (default=1)\n",
    "    回归的目标数目，也就是回归的目标结果的维度，默认是一个标量数字(1维)\n",
    "\n",
    "    @n_features : int, optional (default=100) 特征数\n",
    "    \n",
    "    @noise : float, optional (default=0.0),\n",
    "    生成的数据并不是完全理想的拟合回归模型的数据，而是带有高斯噪声的，noise变量定义了高斯噪声的标准差\n",
    "    The standard deviation of the gaussian noise applied to the output.\n",
    "'''\n",
    "\n",
    "X, Y, Coef = datasets.make_regression(n_samples=200, n_features=1, noise=20, random_state=0, bias=50,coef=True)\n",
    "\n",
    "\n",
    "'''\n",
    "按照一定的比例随机切分训练集和测试集\n",
    "sklearn.model_selection.train_test_split\n",
    "params\n",
    "    @test_size：如果是浮点数，在0-1之间，表示样本占比；如果是整数的话就是样本的数量\n",
    "'''\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.1, random_state=0)\n",
    "X0_train = np.ones([X_train.shape[0],1],dtype=X_train.dtype)\n",
    "X0_test = np.ones([X_test.shape[0],1], dtype=X_test.dtype)\n",
    "X_train_original = copy.deepcopy(X_train)\n",
    "X_train = np.concatenate((X0_train,X_train), axis=1)\n",
    "X_test = np.concatenate((X0_test, X_test), axis=1)\n",
    "\n",
    "\n",
    "# show data\n",
    "plt.scatter(X_test[:,1], Y_test, c='red', edgecolors='white')\n",
    "plt.scatter(X_train[:,1], Y_train, c='green', edgecolors='white')\n",
    "plt.ylim((Y.min()-10, Y.max()+10))\n",
    "plt.xlim((X.min()-1, X.max()+1))\n",
    "plt.show()\n",
    "print('Coef={}'.format(Coef))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-16T00:36:52.210464Z",
     "iopub.status.busy": "2022-08-16T00:36:52.209960Z",
     "iopub.status.idle": "2022-08-16T00:37:06.933254Z",
     "shell.execute_reply": "2022-08-16T00:37:06.931890Z",
     "shell.execute_reply.started": "2022-08-16T00:36:52.210415Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import copy\n",
    "#import pdb\n",
    "#pdb.set_trace()\n",
    "\n",
    "#learnable parameter theta\n",
    "THETA = np.zeros([2,1], dtype=np.float32)\n",
    "# learning rate\n",
    "lr = 0.02\n",
    "epoch = 3\n",
    "\n",
    "#plt.ion()\n",
    "thetas = []\n",
    "ln = None\n",
    "'''\n",
    "linspace,Return evenly spaced numbers over a specified interval.\n",
    "在闭区间[2, 3]生成5个间隔相同的数字\n",
    "print(np.linspace(2.0, 3.0, num=5))\n",
    "[2.   2.25 2.5  2.75 3.  ]\n",
    "'''\n",
    "\n",
    "p_x = np.linspace(X.min()-1, X.max()+1, 50)\n",
    "fig = plt.figure()\n",
    "plt.scatter(X_train[:,1], Y_train, c='purple', marker='o', edgecolors='white')\n",
    "ln, = plt.plot(p_x, THETA[1,0]*p_x+THETA[0,0], c='orange')\n",
    "\n",
    "def init():\n",
    "    plt.ylim((Y.min()-10, Y.max()+10))\n",
    "    plt.xlim((X.min()-1, X.max()+1))    \n",
    "    return ln,\n",
    "\n",
    "'''\n",
    "numpy中的乘法\n",
    "　元素相乘:multply()\n",
    "　矩阵相乘:dot()、matmul()、’@’\n",
    "　’*': 是特别的。在数组操作中，作为元素相乘；在矩阵操作中作为矩阵相乘。\n",
    "batch_X\n",
    "[[ 1.          1.        ]\n",
    "    [ 0.37642553 -1.16514984]]\n",
    "\n",
    "batch-Y\n",
    "[[ 85.83865632]\n",
    "    [-45.48857996]]\n",
    "theta\n",
    "[[0.40350076]\n",
    "    [0.85312873]]\n",
    "loss\n",
    "[[2359.37145637 2359.37145637]]\n",
    "\n",
    "'''\n",
    "def h(theta, X):\n",
    "    h_x = np.matmul(theta.T,X)\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    return h_x\n",
    "\n",
    "def loss_function(theta, X, Y):\n",
    "    cost = 1/2*np.matmul((h(theta,X)-Y.T),(h(X,theta)-Y.T).T)\n",
    "    return cost/(X.shape[0])\n",
    "\n",
    "# child block matrix trick using mulitiply functions\n",
    "def gradient(theta,X, Y):\n",
    "    gradient = np.multiply(X.T,(h(theta,X)-Y.T).T)\n",
    "    return gradient.mean(axis=0).T\n",
    "\n",
    "def update_parameters(theta, gradient, lr):\n",
    "    theta = theta-lr*gradient\n",
    "    return theta\n",
    "\n",
    "def update(frame):\n",
    "    #xdata.append(frame)\n",
    "    #ydata.append(np.shin(frame))\n",
    "    Y = frame[1,0]*p_x+frame[0,0]\n",
    "    ln.set_data(p_x, Y)\n",
    "    return ln,\n",
    "\n",
    "\n",
    "for ep in range(epoch):\n",
    "    for index in range(X_train.shape[0]//2):\n",
    "        batch_X = X_train[index*2:(index+1)*2,:].T # 2*2 matrix\n",
    "        batch_Y = Y_train[index*2:(index+1)*2].reshape([2,1])# 2*1 matrix\n",
    "        h_X = h(THETA, batch_X) # 1*2 matrix\n",
    "        loss = loss_function(THETA, batch_X, batch_Y)\n",
    "        g = gradient(THETA, batch_X, batch_Y)\n",
    "        g = g.reshape([2,1])\n",
    "        THETA = update_parameters(THETA, g, lr)\n",
    "        print(\"[{0}/{1}, Stpes:{2}, loss: {3}, Weight: {4}, Bias:{5}]\".format( \\\n",
    "                   ep+1, epoch, index+1, loss[0][0],THETA[1,0],THETA[0,0]))\n",
    "        theta = copy.deepcopy(THETA)\n",
    "        thetas.append(theta)\n",
    "        #import IPython\n",
    "        #IPython.embed(colors=\"Linux\")\n",
    "        #plt.cla()\n",
    "        #plt.scatter(X_train[:,1], Y_train, c='purple', marker='o', edgecolors='white')\n",
    "        #plt.plot(p_x, THETA[1,0]*p_x+THETA[0,0], c='orange')\n",
    "        #plt.pause(0.01)\n",
    "        # ims.append(im)\n",
    "#plt.ioff()\n",
    "anim = animation.FuncAnimation(fig, update,frames=thetas,interval=25,init_func=init, blit=True)\n",
    "# plt.show()\n",
    "#anim.save(\"/kaggle/working/linear_regression_demo.gif\",writer='pillow')\n",
    "# ani = animation.ArtistAnimation(fig, ims, interval=200, repeat_delay=1000)\n",
    "anim.save(\"/mnt/code/machine-learning/sgd_linear_regression.gif\",writer='pillow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-16T00:37:06.936245Z",
     "iopub.status.busy": "2022-08-16T00:37:06.935828Z",
     "iopub.status.idle": "2022-08-16T00:37:17.621825Z",
     "shell.execute_reply": "2022-08-16T00:37:17.620786Z",
     "shell.execute_reply.started": "2022-08-16T00:37:06.936208Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "train_log_filepath = \"./train_log.txt\"\n",
    "train_log_txt_formatter = \"{time_str} [Epoch] {epoch:03d} [Loss] {loss_str}\\n\"\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LinearRegression(1,1)\n",
    "model = model.double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "num_epoches = 5\n",
    "for epoche in range(num_epoches):\n",
    "    for index in range(X_train_original.shape[0]//2):\n",
    "        batch_X = X_train_original[index*2:(index+1)*2,:]\n",
    "        batch_Y = Y_train[index*2:(index+1)*2].reshape([2,1])\n",
    "        inputs = torch.from_numpy(batch_X)\n",
    "        targets = torch.from_numpy(batch_Y)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #if (epoch+1) % 5 == 0:\n",
    "    print ('Epoch [%d/%d], Loss: %.4f' \n",
    "           %(epoch+1, num_epoches, loss.item()))\n",
    "    to_write = train_log_txt_formatter.format(time_str=time.strftime(\"%Y_%m_%d_%H:%M:%S\"),\n",
    "                                          epoch = epoch,\n",
    "                                          loss_str=loss.item())\n",
    "    with open(train_log_filepath, \"a\") as f:\n",
    "        f.write(to_write)\n",
    "predicted = model(torch.from_numpy(X_train_original)).data.numpy()\n",
    "plt.plot(X_train_original, Y_train, 'ro', label='Original data')\n",
    "plt.plot(X_train_original, predicted, label='Fitted line')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
