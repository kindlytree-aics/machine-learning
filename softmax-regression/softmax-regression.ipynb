{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. \n","In logistic regression we assumed that the labels were binary: $y^{(i)} \\in \\{0,1\\}$. We used such a classifier to distinguish between two kinds of hand-written digits. \n","Softmax regression allows us to handle $y^{(i)} \\in \\{1,\\ldots,K\\} $where K is the number of classes.Recall that in logistic regression, we had a training\n","set $\\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\}$of m labeled examples, where the input features are $x^{(i)} \\in \\Re^{n}$. With logistic regression, we were in the binary classification setting, \n","so the labels were $y^{(i)} \\in \\{0,1\\}$. Our hypothesis took the form:\n","\\begin{align}\n","h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^\\top x)},\n","\\end{align}\n","\n","and the model parameters θ were trained to minimize the cost function\n","\\begin{align}\n","J(\\theta) = -\\left[ \\sum_{i=1}^m y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) \\right]\n","\\end{align}\n","In the softmax regression setting, we are interested in multi-class classification (as opposed to only binary classification), and so the label y can take on K different values, rather than only two. Thus, in our training set $\\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\}$, we now have that $y^{(i)} \\in \\{1, 2, \\ldots, K\\}$. (Note that our convention will be to index the classes starting from 1, rather than from 0.) For example, in the MNIST digit recognition task, we would have K=10 different classes.\n","\n","Given a test input x, we want our hypothesis to estimate the probability that$P(y=k | x)$for each value of k=1,…,K. I.e., we want to estimate the probability of the class label taking on each of the K different possible values. Thus, our hypothesis will output a K-dimensional vector (whose elements sum to 1) giving us our K estimated probabilities. Concretely, our hypothesis $h_\\theta(x)$ takes the form:\n","\\begin{align}\n","h_\\theta(x) =\n","\\begin{bmatrix}\n","P(y = 1 | x; \\theta) \\\\\n","P(y = 2 | x; \\theta) \\\\\n","\\vdots \\\\\n","P(y = K | x; \\theta)\n","\\end{bmatrix}\n","\\end{align}\n","\n","\\begin{align}\n","=\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)\\top} x) }}\n","\\begin{bmatrix}\n","\\exp(\\theta^{(1)\\top} x ) \\\\\n","\\exp(\\theta^{(2)\\top} x ) \\\\\n","\\vdots \\\\\n","\\exp(\\theta^{(K)\\top} x ) \\\\\n","\\end{bmatrix}\n","\\end{align}\n","\n","Here $\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(K)} \\in \\Re^{n}$ are the parameters of our model. Notice that the term $\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)\\top} x) } }$ normalizes the distribution, so that it sums to one.\n","\n","For convenience, we will also write $\\theta$ to denote all the parameters of our model. When you implement softmax regression, it is usually convenient to represent θ as a n-by-K matrix obtained by concatenating $\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(K)} $ into columns, so that\n","\\begin{align}\n","\\theta = \\left[\\begin{array}{cccc}| & | & | & | \\\\\n","\\theta^{(1)} & \\theta^{(2)} & \\cdots & \\theta^{(K)} \\\\\n","| & | & | & 1 \\end{array}\\right]\n","\\end{align}"]},{"cell_type":"markdown","metadata":{},"source":["## 公式推导\n","\n","Let’s look at one more example of a GLM. Consider a classification problem in which the response variable y can take on any one of k values, so y ∈ {1, 2, . . . , k}. For example, rather than classifying email into the two classes\n","spam or not-spam—which would have been a binary classification problem we might want to classify it into three classes, such as spam, personal mail, and work-related mail. The response variable is still discrete, but can now take on more than two values. We will thus model it as distributed according to a multinomial distribution.\n","\n","Let’s derive a GLM for modelling this type of multinomial data. To do so, we will begin by expressing the multinomial(多项的) as an exponential family distribution.\n","\n","To parameterize a multinomial over k possible outcomes, one could use k parameters $φ_1 , . . . , φ_k$ specifying the probability of each of the outcomes.However, these parameters would be redundant, or more formally, they would not be independent (since knowing any\n"," k − 1 of the $φ_i$ ’s uniquely determines the last one, as they must satisfy $\\sum_{i=1}^k\\phi_i=1)$. So, we will instead parameterize the multinomial with only k−1 parameters,$ \\phi_1 , . . . , \\phi_{k−1} $, where$\\phi_i = p(y = i; \\phi)$, and $ p(y = k;\\phi) = 1 −\\sum_{i=1}^{k-1}\\phi_i $. For notational convenience,\n","we will also let$\\phi_k = 1 −\\sum_{i=1}^{k-1}\\phi_i$, but we should keep in mind that this is not a parameter, and that it is fully specified by $\\phi_1 , . . . , \\phi_{k−1}$ .\n","To express the multinomial as an exponential family distribution, we will define $T (y) \\in \\mathbb{R}^{k−1} $as follows:\n","\n","\\begin{align}\n","T(1)=\\begin{bmatrix}\n","1 \\\\\n","0 \\\\\n","0 \\\\\n","\\vdots \\\\\n","0 \\end{bmatrix},\n","T(2)=\\begin{bmatrix}\n","0 \\\\\n","1 \\\\\n","0 \\\\\n","\\vdots \\\\\n","0 \\end{bmatrix},\n","T(3)=\\begin{bmatrix}\n","0 \\\\\n","0 \\\\\n","1 \\\\\n","\\vdots \\\\\n","0 \\end{bmatrix},\n","\\cdots,\n","T(k-1)=\\begin{bmatrix}\n","0 \\\\\n","0 \\\\\n","0 \\\\\n","\\vdots \\\\\n","1 \\end{bmatrix},\n","T(k-1)=\\begin{bmatrix}\n","0 \\\\\n","0 \\\\\n","0 \\\\\n","\\vdots \\\\\n","0 \\end{bmatrix}\n","\\end{align}\n","\n","Unlike our previous examples, here we do not have T (y) = y; also, T (y) is now a k − 1 dimensional vector, rather than a real number. We will write$(T (y))_i$ to denote the i-th element of the vector T (y).\n","\n","We introduce one more very useful piece of notation. An indicator function 1{·} takes on a value of 1 if its argument is true, and 0 otherwise(1{True} = 1, 1{False} = 0). For example, 1{2 = 3} = 0, and 1{3 =5 − 2} = 1. So, we can also write the relationship between T (y) and y as $(T (y))_i = 1\\begin{Bmatrix}y=i\\end{Bmatrix}$. (Before you continue reading, please make sure you understand why this is true!) Further, we have that $E[(T (y))_i] = P (y = i) = \\phi_i$ .\n","\n","We are now ready to show that the multinomial is a member of theexponential family. We have:\n","\\begin{align}\n","p(y;\\phi)=\\phi_1^{1 \\left\\{ y=1 \\right\\}}\\phi_2^{1 \\left\\{ y=2 \\right\\}}\\cdots\\phi_k^{1 \\left\\{ y=k \\right\\}}\n","=\\phi_1^{1 \\left\\{ y=1 \\right\\}}\\phi_2^{1 \\left\\{ y=2 \\right\\}}\\cdots\\phi_k^{1 -\\sum_{i=1}^{k-1}1\\left\\{y=i\\right\\}}\n","=\\phi_1^{(T(y))_1}\\phi_2^{(T(y))_2}\\cdots\\phi_k^{1 -\\sum_{i=1}^{k-1}(T(y))_i}\n","\\end{align}\n","\n","\\begin{align}\n","= exp((T(y))_1log(\\phi_1/\\phi_k)+exp((T(y))_1log(\\phi_2/\\phi_k)+\\cdots+exp((T(y))_{k-1}log(\\phi_{k-1}/\\phi_k)+log(\\phi_k))\n","=b(y)exp(\\eta^TT(y)-a(\\eta))\n","\\end{align}\n","\n","where\n","\n","\\begin{align}\n","\\eta = \\begin{bmatrix}\n","log(\\phi_1/\\phi_k)\\\\\n","log(\\phi_2/\\phi_k)\\\\\n","\\vdots\\\\\n","log(\\phi_{k-1}/\\phi_k) \\end{bmatrix},\n","a(\\eta) = -log(\\phi_k),\n","b(y) = 1\n","\\end{align}\n","\n","This completes our formulation of the multinomial as an exponential family distribution.The link function is given (for i = 1, . . . , k) by\n","\n","$\\eta_i = log\\frac{\\phi_i}{\\phi_k}$\n","For convenience, we have also defined $η_k = log(φ_k/φ_k ) = 0.$ To invert the link function and derive the response function, we therefore have that\n","\n","\n","\\begin{align}\n","e^{\\eta_i} = \\frac {\\phi_i} {\\phi_k},\n","\\phi_ke^{\\eta_i} = \\phi_i,\n","\\phi_k\\sum_{i=1}^ke^{\\eta_i}=\\sum_{i=1}^k\\phi_i=1\n","\\end{align}\n","\n","This implies that$\\phi_k=1/\\sum_{i=1}^ke^{\\eta_i}$,which can be substituted back into  to give the response function\n","\\begin{align}\n","\\phi_k=\\frac {e^{\\eta_i}}{\\sum_{i=1}^ke^{\\eta_i}}\n","\\end{align}\n","\n","This function mapping from the $\\eta$’s to the $\\phi$’s is called the softmax function.\n","\n","To complete our model, we use Assumption 3, given earlier, that the $\\eta_i$’s\n","are linearly related to the x’s. So, have $\\eta_i = \\theta_i^T x (for i = 1, . . . , k − 1)$,\n","where $\\theta_1, . . . , \\theta_k−1 \\in \\mathbb{R}^{d+1} $ are the parameters of our model. For notational\n","convenience, we can also define $\\theta_k = 0$, so that $\\eta_k = \\theta_k^T x = 0$, as given\n","previously. Hence, our model assumes that the conditional distribution of y\n","given x is given by\n","\n","\\begin{align}\n","p(y=i|x;\\theta)=\\phi_i = \\frac {e^{\\eta_i}}{\\sum_{j=1}^k e^{\\eta_j}}=\\frac {e^{\\theta_i^Tx}}{\\sum_{j=1}^k e^{\\theta_j^Tx}}\n","\\end{align}"]},{"cell_type":"markdown","metadata":{},"source":["## 问题求解\n","We now describe the cost function that we’ll use for softmax regression. In the equation below, 1{⋅} is the ”‘indicator function,”’ so that 1{a true statement}=1, and 1{a false statement}=0. For example, 1{2+2=4} evaluates to 1; whereas 1{1+1=5} evaluates to 0. Our cost function will be:\n","\n","\\begin{align}\n","J(\\theta) = - \\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K}  1\\left\\{y^{(i)} = k\\right\\} \\log \\frac{\\exp(\\theta^{(k)\\top} x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)})}\\right]\n","\\end{align}\n","\n","The softmax cost function is similar, except that we now sum over the K different possible values of the class label. Note also that in softmax regression, we have that\n","\n","\\begin{align}\n","P(y^{(i)} = k | x^{(i)} ; \\theta) = \\frac{\\exp(\\theta^{(k)\\top} x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)}) }\n","\\end{align}\n","\n","We cannot solve for the minimum of J(θ) analytically, and thus as usual we’ll resort to an iterative optimization algorithm. Taking derivatives, one can show that the gradient is:\n","\n","\\begin{align}\n","\\nabla_{\\theta^{(k)}} J(\\theta) = - \\sum_{i=1}^{m}{ \\left[ x^{(i)} \\left( 1\\{ y^{(i)} = k\\}  - P(y^{(i)} = k | x^{(i)}; \\theta) \\right) \\right]  }\n","\\end{align}"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# original method, use numpy to implement\n","%matplotlib inline\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from math import exp, log\n","import copy\n","np.random.seed(1)\n","\n","# Hyper-parameters\n","LR = 0.05\n","DECAY_RATE = 0.005\n","THETA = np.random.normal(0, 0.5, 3*3).reshape(3, 3)\n","EPOCH = 20\n","\n","X, Y = make_blobs(n_samples=150, n_features=2, centers=3, random_state=3)\n","# transform y to onehot vector\n","# scikit-learn 中 OneHotEncoder 解析 https://www.cnblogs.com/zhoukui/p/9159909.html\n","encoder = OneHotEncoder(categories='auto')\n","Y = encoder.fit_transform(np.reshape(Y, (150,1))).toarray()\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n","plt.scatter(X_train[:,0], X_train[:, 1], c=np.argmax(Y_train, axis=1), edgecolors='white', marker='s')\n","plt.show()\n","\n","X0_train = np.ones([X_train.shape[0],1],dtype=X_train.dtype)\n","X0_test = np.ones([X_test.shape[0],1], dtype=X_test.dtype)\n","X_train_original = copy.deepcopy(X_train)\n","X_train = np.concatenate((X0_train,X_train), axis=1)\n","X_test = np.concatenate((X0_test, X_test), axis=1)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["THETA [[ 3.05441469  0.77867221  0.56945822]\n"," [ 3.17164649 -2.97672797  5.24549999]\n"," [ 8.56296797  0.81404279  7.53288853]]\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["## 2. Make a hypothesis\n","def hypothesis(x, THETA):\n","    x = np.reshape(x, (3, 1))\n","    temp = np.matmul(THETA.T, x)\n","    temp = np.exp(temp)\n","    denominator = np.sum(temp)\n","    hypothesis = temp / denominator # normalize into 1return \n","    return hypothesis\n","\n","## 3. Loss definition\n","def compute_loss(x, y,THETA):\n","    loss = 0\n","    x = np.reshape(x, (3, 1))\n","    y = np.reshape(y, (3, 1))     # \n","    h_x = hypothesis(x, THETA)    # hypothesis (3, 1)\n","    label = np.argmax(y, axis=0)  # the category of prediction\n","    loss += (-np.log(h_x[label][0] + 0.0000001))  # loss = - y * log(y')\n","    return loss\n","\n","## 4. Parameters updating\n","def update_parameters(THETA, x, y):\n","    x = np.reshape(x, (3, 1))\n","    y = np.reshape(y, (3, 1))\n","    \n","    h_x = hypothesis(x, THETA)\n","\n","    label = np.argmax(y, axis=0)\n","    #print(y, label)\n","    # θk := θk - （-yk * (1/y'k) * x)  k --> the class, yk and y'k are real number, x is a vector \n","    #THETA[:, label] = THETA[:, label] - LR *(-y[label][0] * (1 / h_x[label][0] * x))\n","    #HETA[:, label] = THETA[:, label] - LR *(1-h_x[label][0])x\n","    #print(h_x)\n","    x = np.reshape(x,[3,1])\n","    #print(THETA[:, label].shape)\n","    THETA[:, label] = THETA[:, label]+LR*(1-h_x[label][0])*x\n","    return THETA\n","\n","for epoch in range(EPOCH):\n","    LR = LR * (1 / (1 + DECAY_RATE * epoch))\n","    i = 0 # retrieve H_x\n","    for x,y in zip(X_train,Y_train):\n","        loss = compute_loss(x, y, THETA)\n","        #print('[{0}/{1}] loss is: {2}'.format(epoch+1, EPOCH, loss))\n","        THETA = update_parameters(THETA, x, y)\n","\n","i = 0\n","print('THETA', THETA)\n","H_test = np.zeros((Y_test.shape[0], Y_test.shape[1]))\n","#H_test = np.zeros([Y_test.shape[0], 1], dtype=Y_test.dtype)\n","for x, y in zip(X_test, Y_test):\n","    H_test[i] = hypothesis(x, THETA).T\n","    i+=1\n","plt.figure(1)\n","x = np.linspace(-7, 4, 50)\n","plt.scatter(X_test[:, 1], X_test[:, 2], c=np.argmax(H_test, axis=1), edgecolors='white', marker='s')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["References:\n","- http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/\n","- "]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}}},"nbformat":4,"nbformat_minor":4}
