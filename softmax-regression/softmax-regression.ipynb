{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. \n","In logistic regression we assumed that the labels were binary: $y^{(i)} \\in \\{0,1\\}$. We used such a classifier to distinguish between two kinds of hand-written digits. \n","Softmax regression allows us to handle $y^{(i)} \\in \\{1,\\ldots,K\\} $where K is the number of classes.Recall that in logistic regression, we had a training\n","set $\\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\}$of m labeled examples, where the input features are $x^{(i)} \\in \\Re^{n}$. With logistic regression, we were in the binary classification setting, \n","so the labels were $y^{(i)} \\in \\{0,1\\}$. Our hypothesis took the form:\n","\\begin{align}\n","h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^\\top x)},\n","\\end{align}\n","\n","and the model parameters θ were trained to minimize the cost function\n","\\begin{align}\n","J(\\theta) = -\\left[ \\sum_{i=1}^m y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) \\right]\n","\\end{align}\n","In the softmax regression setting, we are interested in multi-class classification (as opposed to only binary classification), and so the label y can take on K different values, rather than only two. Thus, in our training set $\\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\}$, we now have that $y^{(i)} \\in \\{1, 2, \\ldots, K\\}$. (Note that our convention will be to index the classes starting from 1, rather than from 0.) For example, in the MNIST digit recognition task, we would have K=10 different classes.\n","\n","Given a test input x, we want our hypothesis to estimate the probability that$P(y=k | x)$for each value of k=1,…,K. I.e., we want to estimate the probability of the class label taking on each of the K different possible values. Thus, our hypothesis will output a K-dimensional vector (whose elements sum to 1) giving us our K estimated probabilities. Concretely, our hypothesis $h_\\theta(x)$ takes the form:\n","\\begin{align}\n","h_\\theta(x) =\n","\\begin{bmatrix}\n","P(y = 1 | x; \\theta) \\\\\n","P(y = 2 | x; \\theta) \\\\\n","\\vdots \\\\\n","P(y = K | x; \\theta)\n","\\end{bmatrix}\n","\\end{align}\n","\n","\\begin{align}\n","=\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)\\top} x) }}\n","\\begin{bmatrix}\n","\\exp(\\theta^{(1)\\top} x ) \\\\\n","\\exp(\\theta^{(2)\\top} x ) \\\\\n","\\vdots \\\\\n","\\exp(\\theta^{(K)\\top} x ) \\\\\n","\\end{bmatrix}\n","\\end{align}\n","\n","Here $\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(K)} \\in \\Re^{n}$ are the parameters of our model. Notice that the term $\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)\\top} x) } }$ normalizes the distribution, so that it sums to one.\n","\n","For convenience, we will also write $\\theta$ to denote all the parameters of our model. When you implement softmax regression, it is usually convenient to represent θ as a n-by-K matrix obtained by concatenating $\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(K)} $ into columns, so that\n","\\begin{align}\n","\\theta = \\left[\\begin{array}{cccc}| & | & | & | \\\\\n","\\theta^{(1)} & \\theta^{(2)} & \\cdots & \\theta^{(K)} \\\\\n","| & | & | & 1 \\end{array}\\right]\n","\\end{align}"]},{"cell_type":"markdown","metadata":{},"source":["## 公式推导\n","\n","Let’s look at one more example of a GLM. Consider a classification problem in which the response variable y can take on any one of k values, so y ∈ {1, 2, . . . , k}. For example, rather than classifying email into the two classes\n","spam or not-spam—which would have been a binary classification problem we might want to classify it into three classes, such as spam, personal mail, and work-related mail. The response variable is still discrete, but can now take on more than two values. We will thus model it as distributed according to a multinomial distribution.\n","\n","Let’s derive a GLM for modelling this type of multinomial data. To do so, we will begin by expressing the multinomial(多项的) as an exponential family distribution.\n","\n","To parameterize a multinomial over k possible outcomes, one could use k parameters $φ_1 , . . . , φ_k$ specifying the probability of each of the outcomes.However, these parameters would be redundant, or more formally, they would not be independent (since knowing any\n"," k − 1 of the $φ_i$ ’s uniquely determines the last one, as they must satisfy $\\sum_{i=1}^k\\phi_i=1)$. So, we will instead parameterize the multinomial with only k−1 parameters,$ \\phi_1 , . . . , \\phi_{k−1} $, where$\\phi_i = p(y = i; \\phi)$, and $ p(y = k;\\phi) = 1 −\\sum_{i=1}^{k-1}\\phi_i $. For notational convenience,\n","we will also let$\\phi_k = 1 −\\sum_{i=1}^{k-1}\\phi_i$, but we should keep in mind that this is not a parameter, and that it is fully specified by $\\phi_1 , . . . , \\phi_{k−1}$ .\n","To express the multinomial as an exponential family distribution, we will define $T (y) \\in \\mathbb{R}^{k−1} $as follows:\n","\n","\\begin{align}\n","T(1)=\\begin{bmatrix}\n","1 \\\\\n","0 \\\\\n","0 \\\\\n","\\vdots \\\\\n","0 \\end{bmatrix},\n","T(2)=\\begin{bmatrix}\n","0 \\\\\n","1 \\\\\n","0 \\\\\n","\\vdots \\\\\n","0 \\end{bmatrix},\n","T(3)=\\begin{bmatrix}\n","0 \\\\\n","0 \\\\\n","1 \\\\\n","\\vdots \\\\\n","0 \\end{bmatrix},\n","\\cdots,\n","T(k-1)=\\begin{bmatrix}\n","0 \\\\\n","0 \\\\\n","0 \\\\\n","\\vdots \\\\\n","1 \\end{bmatrix},\n","T(k-1)=\\begin{bmatrix}\n","0 \\\\\n","0 \\\\\n","0 \\\\\n","\\vdots \\\\\n","0 \\end{bmatrix}\n","\\end{align}\n","\n","Unlike our previous examples, here we do not have T (y) = y; also, T (y) is now a k − 1 dimensional vector, rather than a real number. We will write$(T (y))_i$ to denote the i-th element of the vector T (y).\n","\n","We introduce one more very useful piece of notation. An indicator function 1{·} takes on a value of 1 if its argument is true, and 0 otherwise(1{True} = 1, 1{False} = 0). For example, 1{2 = 3} = 0, and 1{3 =5 − 2} = 1. So, we can also write the relationship between T (y) and y as $(T (y))_i = 1\\begin{Bmatrix}y=i\\end{Bmatrix}$. (Before you continue reading, please make sure you understand why this is true!) Further, we have that $E[(T (y))_i] = P (y = i) = \\phi_i$ .\n","\n","We are now ready to show that the multinomial is a member of theexponential family. We have:\n","\\begin{align}\n","p(y;\\phi)=\\phi_1^{1 \\left\\{ y=1 \\right\\}}\\phi_2^{1 \\left\\{ y=2 \\right\\}}\\cdots\\phi_k^{1 \\left\\{ y=k \\right\\}}\n","=\\phi_1^{1 \\left\\{ y=1 \\right\\}}\\phi_2^{1 \\left\\{ y=2 \\right\\}}\\cdots\\phi_k^{1 -\\sum_{i=1}^{k-1}1\\left\\{y=i\\right\\}}\n","=\\phi_1^{(T(y))_1}\\phi_2^{(T(y))_2}\\cdots\\phi_k^{1 -\\sum_{i=1}^{k-1}(T(y))_i}\n","\\end{align}\n","\n","\\begin{align}\n","= exp((T(y))_1log(\\phi_1/\\phi_k)+exp((T(y))_1log(\\phi_2/\\phi_k)+\\cdots+exp((T(y))_{k-1}log(\\phi_{k-1}/\\phi_k)+log(\\phi_k))\n","=b(y)exp(\\eta^TT(y)-a(\\eta))\n","\\end{align}\n","\n","where\n","\n","\\begin{align}\n","\\eta = \\begin{bmatrix}\n","log(\\phi_1/\\phi_k)\\\\\n","log(\\phi_2/\\phi_k)\\\\\n","\\vdots\\\\\n","log(\\phi_{k-1}/\\phi_k) \\end{bmatrix},\n","a(\\eta) = -log(\\phi_k),\n","b(y) = 1\n","\\end{align}\n","\n","This completes our formulation of the multinomial as an exponential family distribution.The link function is given (for i = 1, . . . , k) by\n","\n","$\\eta_i = log\\frac{\\phi_i}{\\phi_k}$\n","For convenience, we have also defined $η_k = log(φ_k/φ_k ) = 0.$ To invert the link function and derive the response function, we therefore have that\n","\n","\n","\\begin{align}\n","e^{\\eta_i} = \\frac {\\phi_i} {\\phi_k},\n","\\phi_ke^{\\eta_i} = \\phi_i,\n","\\phi_k\\sum_{i=1}^ke^{\\eta_i}=\\sum_{i=1}^k\\phi_i=1\n","\\end{align}\n","\n","This implies that$\\phi_k=1/\\sum_{i=1}^ke^{\\eta_i}$,which can be substituted back into  to give the response function\n","\\begin{align}\n","\\phi_k=\\frac {e^{\\eta_i}}{\\sum_{i=1}^ke^{\\eta_i}}\n","\\end{align}\n","\n","This function mapping from the $\\eta$’s to the $\\phi$’s is called the softmax function.\n","\n","To complete our model, we use Assumption 3, given earlier, that the $\\eta_i$’s\n","are linearly related to the x’s. So, have $\\eta_i = \\theta_i^T x (for i = 1, . . . , k − 1)$,\n","where $\\theta_1, . . . , \\theta_k−1 \\in \\mathbb{R}^{d+1} $ are the parameters of our model. For notational\n","convenience, we can also define $\\theta_k = 0$, so that $\\eta_k = \\theta_k^T x = 0$, as given\n","previously. Hence, our model assumes that the conditional distribution of y\n","given x is given by\n","\n","\\begin{align}\n","p(y=i|x;\\theta)=\\phi_i = \\frac {e^{\\eta_i}}{\\sum_{j=1}^k e^{\\eta_j}}=\\frac {e^{\\theta_i^Tx}}{\\sum_{j=1}^k e^{\\theta_j^Tx}}\n","\\end{align}"]},{"cell_type":"markdown","metadata":{},"source":["## 问题求解\n","We now describe the cost function that we’ll use for softmax regression. In the equation below, 1{⋅} is the ”‘indicator function,”’ so that 1{a true statement}=1, and 1{a false statement}=0. For example, 1{2+2=4} evaluates to 1; whereas 1{1+1=5} evaluates to 0. Our cost function will be:\n","\n","\\begin{align}\n","J(\\theta) = - \\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K}  1\\left\\{y^{(i)} = k\\right\\} \\log \\frac{\\exp(\\theta^{(k)\\top} x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)})}\\right]\n","\\end{align}\n","\n","The softmax cost function is similar, except that we now sum over the K different possible values of the class label. Note also that in softmax regression, we have that\n","\n","\\begin{align}\n","P(y^{(i)} = k | x^{(i)} ; \\theta) = \\frac{\\exp(\\theta^{(k)\\top} x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)}) }\n","\\end{align}\n","\n","We cannot solve for the minimum of J(θ) analytically, and thus as usual we’ll resort to an iterative optimization algorithm. Taking derivatives, one can show that the gradient is:\n","\n","\\begin{align}\n","\\nabla_{\\theta^{(k)}} J(\\theta) = - \\sum_{i=1}^{m}{ \\left[ x^{(i)} \\left( 1\\{ y^{(i)} = k\\}  - P(y^{(i)} = k | x^{(i)}; \\theta) \\right) \\right]  }\n","\\end{align}"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'EPOCH' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\work\\code\\kindlytree_aics\\machine-learning\\softmax-regression\\softmax-regression.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/work/code/kindlytree_aics/machine-learning/softmax-regression/softmax-regression.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     THETA[:, label] \u001b[39m=\u001b[39m THETA[:, label]\u001b[39m+\u001b[39mLR\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mh_x[label][\u001b[39m0\u001b[39m])\u001b[39m*\u001b[39mx\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/work/code/kindlytree_aics/machine-learning/softmax-regression/softmax-regression.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m THETA\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/work/code/kindlytree_aics/machine-learning/softmax-regression/softmax-regression.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCH):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/work/code/kindlytree_aics/machine-learning/softmax-regression/softmax-regression.ipynb#W3sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     LR \u001b[39m=\u001b[39m LR \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m DECAY_RATE \u001b[39m*\u001b[39m epoch))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/work/code/kindlytree_aics/machine-learning/softmax-regression/softmax-regression.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# retrieve H_x\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'EPOCH' is not defined"]}],"source":["## 2. Make a hypothesis\n","def hypothesis(x, THETA):\n","    x = np.reshape(x, (3, 1))\n","    temp = np.matmul(THETA.T, x)\n","    temp = np.exp(temp)\n","    denominator = np.sum(temp)\n","    hypothesis = temp / denominator # normalize into 1return \n","    return hypothesis\n","\n","## 3. Loss definition\n","def compute_loss(x, y,THETA):\n","    loss = 0\n","    x = np.reshape(x, (3, 1))\n","    y = np.reshape(y, (3, 1))     # \n","    h_x = hypothesis(x, THETA)    # hypothesis (3, 1)\n","    label = np.argmax(y, axis=0)  # the category of prediction\n","    loss += (-np.log(h_x[label][0] + 0.0000001))  # loss = - y * log(y')\n","    return loss\n","\n","## 4. Parameters updating\n","def update_parameters(THETA, x, y):\n","    x = np.reshape(x, (3, 1))\n","    y = np.reshape(y, (3, 1))\n","    \n","    h_x = hypothesis(x, THETA)\n","\n","    label = np.argmax(y, axis=0)\n","    #print(y, label)\n","    # θk := θk - （-yk * (1/y'k) * x)  k --> the class, yk and y'k are real number, x is a vector \n","    #THETA[:, label] = THETA[:, label] - LR *(-y[label][0] * (1 / h_x[label][0] * x))\n","    #HETA[:, label] = THETA[:, label] - LR *(1-h_x[label][0])x\n","    #print(h_x)\n","    x = np.reshape(x,[3,1])\n","    #print(THETA[:, label].shape)\n","    THETA[:, label] = THETA[:, label]+LR*(1-h_x[label][0])*x\n","    return THETA\n","\n","for epoch in range(EPOCH):\n","    LR = LR * (1 / (1 + DECAY_RATE * epoch))\n","    i = 0 # retrieve H_x\n","    for x,y in zip(X_train,Y_train):\n","        loss = compute_loss(x, y, THETA)\n","        #print('[{0}/{1}] loss is: {2}'.format(epoch+1, EPOCH, loss))\n","        THETA = update_parameters(THETA, x, y)\n","\n","i = 0\n","print('THETA', THETA)\n","H_test = np.zeros((Y_test.shape[0], Y_test.shape[1]))\n","#H_test = np.zeros([Y_test.shape[0], 1], dtype=Y_test.dtype)\n","for x, y in zip(X_test, Y_test):\n","    H_test[i] = hypothesis(x, THETA).T\n","    i+=1\n","plt.figure(1)\n","x = np.linspace(-7, 4, 50)\n","plt.scatter(X_test[:, 1], X_test[:, 2], c=np.argmax(H_test, axis=1), edgecolors='white', marker='s')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["References:\n","- http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/\n","- "]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}}},"nbformat":4,"nbformat_minor":4}
